{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc11419b-0ffe-4beb-854c-fd0336c1eeb6",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fbc2eb-57c5-4e65-abbe-87dc0bc787d7",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 5: Policy improving gradients</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea818474-994d-4898-a8a0-4dff51c899af",
   "metadata": {},
   "source": [
    "So far, we have been mostly concerned with the problem of function approximation, that is \"how do we store $Q(s,a)$ in a convenient way, that is amenable to learning, retrieval and optimization?\". A specific feature of the problems we tackled was that they had few discrete actions, making the dependency on $s$ the key difficulty. In particular, when looking for a $Q$-greedy action, the $\\max_{a\\in A}$ problem had a straightforward solution as we could iterate through actions and retain the best one. We now turn to a more general case where the actions are too numerous to be enumerated (either because the action space is continuous or because it just has too many actions). Too many states motivated the introduction of function approximators for $V$ and $Q$; too many actions similarly lead to function approximation for $\\pi$. In the present chapter, we consider the general policy space (with possible approximation) and ask the question \"how do we find a monotonically improving sequence of policies?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c3ea2-ff63-4a30-8a8b-e7a00122f88d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ae975-892e-459b-98bb-8c854d00c13a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
